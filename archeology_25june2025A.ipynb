{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Various_scripts/blob/main/archeology_25june2025A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJHo6-Sy4sLQ"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='velvety-ring-328419')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kxc89j1r8vhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on similar location - modified one to include weights collecting different geographic layers\n",
        "# to provide to the deep learning model for training\n",
        "import geemap\n",
        "\n",
        "# Define region (Brazil, Southeastern Amazon). You can also expand it to other countries\n",
        "mask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_na', 'Brazil'))\n",
        "brazil_geometry = mask.geometry()\n",
        "southeast_brazil = brazil_geometry.intersection(ee.Geometry.Rectangle([-55, -25, -40, -35]))\n",
        "# weights can be adjusted during the model training. The task can be automated using an optimization algorithms\n",
        "weights = {\n",
        "    \"B12\": 0.10,\n",
        "    \"B11\": 0.10,\n",
        "    \"B8\": 0.10,\n",
        "    \"VV\": 0.10,\n",
        "    \"NDVI\": 0.15,\n",
        "    \"Elevation\": 0.15,\n",
        "    \"SoilMoisture\": 0.10,\n",
        "    \"Thermal\": 0.1,\n",
        "    \"Evapotranspiration\": 0.1\n",
        "}\n",
        "def normalize_band(image, band):\n",
        "    band_min = image.select(band).reduceRegion(\n",
        "        reducer=ee.Reducer.min(), geometry=image.geometry(), scale=30, bestEffort=True\n",
        "    ).get(band)\n",
        "\n",
        "    band_max = image.select(band).reduceRegion(\n",
        "        reducer=ee.Reducer.max(), geometry=image.geometry(), scale=30, bestEffort=True\n",
        "    ).get(band)\n",
        "\n",
        "    normalized_band = image.select(band).subtract(ee.Number(band_min)).divide(ee.Number(band_max).subtract(ee.Number(band_min)))\n",
        "    return normalized_band.rename(band + \"_norm\")\n",
        "# Load archaeological points\n",
        "archaeology_points = ee.FeatureCollection(\"users/mohamadawadlebanon/Archeologicalsites\")\n",
        "\n",
        "# Define date range\n",
        "start_date = \"2024-05-01\"\n",
        "end_date = \"2024-05-31\"\n",
        "def apply_scale_and_offset(image):\n",
        "    return image.select(\"ST_B10\").multiply(0.00341802).add(149.0)\n",
        "### **Step 1: Elevation Data (ASTER GDEM)**\n",
        "elevation = ee.Image(\"projects/sat-io/open-datasets/ASTER/GDEM\").clip(southeast_brazil)\n",
        "#elevation = elevation.rename(\"Elevation\")\n",
        "elevation = elevation.rename(\"elevation\")\n",
        "### **Step 2: Sentinel-2 EVI**\n",
        "sentinel2_ndvi = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B4\"]) \\\n",
        "    .map(lambda img: img.expression(\n",
        "        \"(B8 - B4) / (B8 + B4)\",  # EVI formula\n",
        "        {\"B8\": img.select(\"B8\"), \"B4\": img.select(\"B4\")}\n",
        "    )).median().clip(southeast_brazil)\n",
        "sentinel2_ndvi = sentinel2_ndvi.rename(\"NDVI\")\n",
        "\n",
        "### **Step 3: Sentinel-1 Radar**\n",
        "sentinel1_images = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\")) \\\n",
        "    .filter(ee.Filter.eq(\"instrumentMode\", \"IW\")) \\\n",
        "    .select(\"VV\")\n",
        "\n",
        "mosaiced_sentinel1 = sentinel1_images.median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 4: Sentinel-2 Optical Mosaic**\n",
        "sentinel2_images = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B11\", \"B12\"])\n",
        "\n",
        "mosaiced_sentinel2 = sentinel2_images.median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 5: MODIS Evapotranspiration (ET)**\n",
        "modis_et = ee.ImageCollection(\"MODIS/061/MOD16A2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select(\"ET\") \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 6: Soil Moisture (NASA SMAP)**\n",
        "soil_moisture = ee.ImageCollection('NASA/SMAP/SPL4SMGP/007') \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select('sm_surface') \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 7: Thermal Infrared (Landsat 8-9)**\n",
        "thermal = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .map(apply_scale_and_offset) \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "\n",
        "#*** step 8 Normalize all bands\n",
        "\n",
        "normalized_sentinel1 = normalize_band(mosaiced_sentinel1, \"VV\")\n",
        "normalized_ndvi = normalize_band(sentinel2_ndvi, \"NDVI\")\n",
        "normalized_elevation = normalize_band(elevation, \"elevation\")\n",
        "normalized_soil_moisture = normalize_band(soil_moisture, \"sm_surface\")\n",
        "normalized_thermal = normalize_band(thermal, \"ST_B10\")\n",
        "normalized_et = normalize_band(modis_et, \"ET\")\n",
        "\n",
        "### merge all bands and layers\n",
        "combined = mosaiced_sentinel2.select(\"B12\").multiply(weights[\"B12\"]) \\\n",
        ".addBands(mosaiced_sentinel2.select(\"B11\").multiply(weights[\"B11\"])) \\\n",
        ".addBands(mosaiced_sentinel2.select(\"B8\").multiply(weights[\"B8\"])) \\\n",
        ".addBands(mosaiced_sentinel1.multiply(weights[\"VV\"])) \\\n",
        ".addBands(sentinel2_ndvi.multiply(weights[\"NDVI\"])) \\\n",
        ".addBands(elevation.multiply(weights[\"Elevation\"])) \\\n",
        ".addBands(soil_moisture.multiply(weights[\"SoilMoisture\"])) \\\n",
        ".addBands(thermal.multiply(weights[\"Thermal\"])) \\\n",
        ".addBands(modis_et.multiply(weights[\"Evapotranspiration\"]))\n",
        "\n",
        "\n",
        "# Sample feature values at archaeology site locations\n",
        "sampled_values = combined.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 10,  # Adjust scale depending on resolution needs\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "# Convert sampled values to FeatureCollection table\n",
        "def safe_set_coordinates(feature):\n",
        "    return feature.set({\n",
        "        \"Longitude\": feature.get(\"lon\"),\n",
        "        \"Latitude\": feature.get(\"lat\")\n",
        "    })\n",
        "\n",
        "#table = sampled_values.map(safe_set_coordinates)\n",
        "\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=sampled_values, #table,\n",
        "    description=\"Archaeological_Site_Features\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task.start()  # Start export task\n",
        "\n",
        "### **Step 10: Apply K-Means Clustering**\n",
        "num_classes = 10\n",
        "training_points = combined.sample(\n",
        "    region=southeast_brazil,\n",
        "    # Default (False) is no geometries in the output.\n",
        "    # When set to True, each feature has a Point geometry at the center of the\n",
        "    # image pixel.\n",
        "    geometries=True,\n",
        "    numPixels = 500,\n",
        "    # The scale is not specified, so the resolution of the image will be used,\n",
        "    # and there is a feature for every pixel. If we give a scale parameter, the\n",
        "    # image will be resampled and there will be more or fewer features.\n",
        "    #\n",
        "    scale= 10,\n",
        ")\n",
        "# Extract longitude and latitude from geometry\n",
        "\n",
        "#training_points = training_points.filter(ee.Filter.notNull(['geometry']))\n",
        "print(training_points.first().getInfo())\n",
        "task1 = ee.batch.Export.table.toDrive(\n",
        "    collection=training_points,\n",
        "    description=\"Samples_random_collection\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task1.start()\n",
        "clusterer = ee.Clusterer.wekaKMeans(num_classes).train(training_points)\n",
        "classified = combined.cluster(clusterer)\n",
        "\n",
        "### **Step 11: Validate Using Archaeological Sites**\n",
        "validation = classified.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 10,\n",
        "    \"properties\": [\"class\"],\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "\n",
        "### **Step 12: Visualization Parameters**\n",
        "vis_params_elevation = {\"bands\": [\"elevation\"], \"min\": 0, \"max\": 3000, \"palette\": [\"black\", \"white\"]}\n",
        "vis_params_ndvi = {\"bands\": [\"NDVI\"],\"min\": -0.6, \"max\": 0.6, \"palette\": [\"brown\", \"green\"]}\n",
        "vis_params_s1 = {\"bands\": [\"VV\"], \"min\": -20, \"max\": 0, \"gamma\": 1.4}\n",
        "vis_params_s2 = {\"bands\": [\"B12\", \"B11\", \"B8\"], \"min\": 0, \"max\": 3000, \"gamma\": 1.4}\n",
        "vis_params_modis_et = {\"bands\": [\"ET\"], \"min\": 0, \"max\": 300, \"palette\": [\"yellow\", \"green\", \"blue\"]}\n",
        "#vis_params_pml_et = {\"min\": 0, \"max\": 5, \"palette\": [\"orange\", \"red\", \"purple\"]}\n",
        "vis_params_soil = {\"bands\": [\"sm_surface\"], \"min\": 0, \"max\": 0.9, \"palette\": [\"red\", \"orange\", \"yellow\", \"green\"]}\n",
        "vis_params_thermal = {\"bands\": [\"ST_B10\"], \"min\": 270, \"max\": 320, \"palette\": [\"blue\", \"yellow\", \"red\"]}\n",
        "vis_params_classified = {\n",
        "    \"min\": 0,\n",
        "    \"max\": num_classes - 1,\n",
        "    \"palette\": [\"blue\", \"green\", \"yellow\", \"red\", \"purple\", \"orange\", \"brown\", \"cyan\", \"pink\", \"gray\"]\n",
        "}\n",
        "archaeology_vis = {\"color\": \"blue\", \"pointRadius\": 5}\n",
        "sample_vis = {\"color\": \"black\", \"pointRadius\": 4}\n",
        "### **Step 13: Create & Display Map**\n",
        "m = geemap.Map(center=[-3, -60], zoom=6)\n",
        "\n",
        "m.addLayer(elevation, vis_params_elevation, \"ASTER GDEM v3 Elevation\")\n",
        "m.addLayer(sentinel2_ndvi, vis_params_ndvi, \"High-Resolution NDVI (Sentinel-2)\")\n",
        "m.addLayer(mosaiced_sentinel1, vis_params_s1, \"Sentinel-1 Mosaic (Radar)\")\n",
        "m.addLayer(mosaiced_sentinel2, vis_params_s2, \"Sentinel-2 Mosaic (Optical)\")\n",
        "m.addLayer(modis_et, vis_params_modis_et, \"MODIS Evapotranspiration\")\n",
        "#m.addLayer(pml_et, vis_params_pml_et, \"PML Evapotranspiration\")\n",
        "m.addLayer(soil_moisture, vis_params_soil, \"Soil Moisture (SMAP)\")\n",
        "m.addLayer(thermal, vis_params_thermal, \"Thermal Infrared (Landsat)\")\n",
        "m.addLayer(classified, vis_params_classified, \"Classified Image (K-Means)\")\n",
        "m.addLayer(archaeology_points, archaeology_vis, \"Archaeology Points\")\n",
        "m.addLayer(training_points,sample_vis, \"sample Points\")\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "fzW_axLw3MBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geedim"
      ],
      "metadata": {
        "id": "DLDAG4TRSmBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import geemap\n",
        "import os\n",
        "\n",
        "# Set area and output parameters\n",
        "region = southeast_brazil\n",
        "scale = 100  # You can fine-tune for better resolution\n",
        "output_dir = 'exported_maps'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Ordered thematic layers: (filename prefix, ee.Image or ee.FeatureCollection, visParams)\n",
        "layers_to_export = [\n",
        "    (\"1_World_Map\", ee.Image().paint(ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\"), 0, 2), {\"palette\": [\"white\"]}),\n",
        "    (\"2_Study_Area\", ee.Image().paint(southeast_brazil, 0, 3), {\"palette\": [\"red\"]}),\n",
        "    (\"3_Archaeology_Sites\", ee.Image().paint(archaeology_points, 0, 4), {\"palette\": [\"blue\"]}),\n",
        "    (\"4_Sentinel2_RGB\", mosaiced_sentinel2.visualize(**vis_params_s2), {}),\n",
        "    (\"5_Sentinel1_Radar\", mosaiced_sentinel1.visualize(**vis_params_s1), {}),\n",
        "    (\"6_NDVI\", sentinel2_ndvi.visualize(**vis_params_ndvi), {}),\n",
        "    (\"7_Elevation\", elevation.visualize(**vis_params_elevation), {}),\n",
        "    (\"8_Thermal\", thermal.visualize(**vis_params_thermal), {}),\n",
        "    (\"9_Soil_Moisture\", soil_moisture.visualize(**vis_params_soil), {}),\n",
        "    (\"10_Segmented\", classified.visualize(**vis_params_classified), {}),\n",
        "    (\"11_Predicted_Points\", ee.Image().paint(training_points, 0, 5), {\"palette\": [\"black\"]})\n",
        "]\n",
        "crs = 'EPSG:4326'  # WGS 84, standard geographic projection\n",
        "\n",
        "for name, image, vis in layers_to_export:\n",
        "    path = os.path.join(output_dir, f\"{name}.png\")\n",
        "\n",
        "    # Handle visualization if not already rendered\n",
        "    if vis:\n",
        "        image = image.visualize(**vis)\n",
        "\n",
        "    # Export with explicit CRS and scale\n",
        "    geemap.download_ee_image(\n",
        "        image=image,\n",
        "        region=region,\n",
        "        filename=path,\n",
        "        scale=scale,\n",
        "        crs=crs\n",
        "    )\n",
        "    print(f\"Exported {path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ATlaONu7NaHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import geemap\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from PIL import Image\n",
        "\n",
        "# Parameters\n",
        "output_dir = 'exported_maps'\n",
        "region = southeast_brazil\n",
        "scale = 100\n",
        "crs = 'EPSG:4326'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Utility function: Convert GeoTIFF to displayable PNG\n",
        "def convert_to_png(input_tif, output_png):\n",
        "    with rasterio.open(input_tif) as src:\n",
        "        arr = src.read(1)\n",
        "        arr = np.where(arr == src.nodata, np.nan, arr)\n",
        "        # Normalize to 0â€“255\n",
        "        norm = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
        "        norm = np.nan_to_num(norm)\n",
        "        img = (norm * 255).astype(np.uint8)\n",
        "        Image.fromarray(img).save(output_png)\n",
        "\n",
        "# Layers to export\n",
        "layers_to_export = [\n",
        "    (\"1_World_Map\", ee.Image().paint(ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\"), 255, 2), {\"palette\": [\"#d3d3d3\"]}),\n",
        "    (\"2_Study_Area\", ee.Image().paint(southeast_brazil, 255, 3), {\"palette\": [\"#ff0000\"]}),\n",
        "    (\"3_Archaeology_Sites\", ee.Image().paint(archaeology_points, 255, 4), {\"palette\": [\"#0000ff\"]}),\n",
        "    (\"4_Sentinel2_RGB\", mosaiced_sentinel2.visualize(**vis_params_s2), {}),\n",
        "    (\"5_Sentinel1_Radar\", mosaiced_sentinel1.visualize(**vis_params_s1), {}),\n",
        "    (\"6_NDVI\", sentinel2_ndvi.visualize(**vis_params_ndvi), {}),\n",
        "    (\"7_Elevation\", elevation.visualize(**vis_params_elevation), {}),\n",
        "    (\"8_Thermal\", thermal.visualize(**vis_params_thermal), {}),\n",
        "    (\"9_Soil_Moisture\", soil_moisture.visualize(**vis_params_soil), {}),\n",
        "    (\"10_Segmented\", classified.visualize(**vis_params_classified), {}),\n",
        "    (\"11_Predicted_Points\", ee.Image().paint(training_points, 255, 5), {\"palette\": [\"#000000\"]})\n",
        "]\n",
        "\n",
        "for name, image, vis in layers_to_export:\n",
        "    tif_path = os.path.join(output_dir, f\"{name}.tif\")\n",
        "    png_path = os.path.join(output_dir, f\"{name}.png\")\n",
        "\n",
        "    # Apply visualization if needed\n",
        "    if vis:\n",
        "        image = image.visualize(**vis)\n",
        "\n",
        "    # Export GeoTIFF\n",
        "    geemap.download_ee_image(\n",
        "        image=image,\n",
        "        region=region,\n",
        "        filename=tif_path,\n",
        "        scale=scale,\n",
        "        crs=crs\n",
        "    )\n",
        "\n",
        "    # Convert to true PNG for compatibility\n",
        "    convert_to_png(tif_path, png_path)\n",
        "    print(f\"âœ… Saved: {png_path}\")\n"
      ],
      "metadata": {
        "id": "SxhGK_t9iZ8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import geemap\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Setup\n",
        "output_dir = 'exported_maps'\n",
        "region = southeast_brazil\n",
        "scale = 100\n",
        "crs = 'EPSG:4326'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Utility: Convert raster to PNG with optional colormap\n",
        "def stretch_and_convert(tif_path, png_path, cmap='viridis'):\n",
        "    with rasterio.open(tif_path) as src:\n",
        "        arr = src.read(1)\n",
        "        arr = np.where(arr == src.nodata, np.nan, arr)\n",
        "        norm = (arr - np.nanmin(arr)) / (np.nanmax(arr) - np.nanmin(arr))\n",
        "        norm = np.nan_to_num(norm)\n",
        "        color_map = plt.get_cmap(cmap)\n",
        "        rgb = (color_map(norm)[:, :, :3] * 255).astype(np.uint8)\n",
        "        Image.fromarray(rgb).save(png_path)\n",
        "\n",
        "# Define raw layers\n",
        "raw_layers = {\n",
        "    \"World_Borders\": ee.Image().paint(ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\"), 1, 2),\n",
        "    \"Study_Area\": ee.Image().paint(southeast_brazil, 1, 3),\n",
        "    \"Archaeology_Sites\": ee.Image().paint(archaeology_points, 1, 4),\n",
        "    \"Sentinel2_Bands\": mosaiced_sentinel2.select(\"B12\"),\n",
        "    \"Sentinel1_VV\": mosaiced_sentinel1.select(\"VV\"),\n",
        "    \"NDVI\": sentinel2_ndvi,\n",
        "    \"Elevation\": elevation.select(\"elevation\"),\n",
        "    \"Thermal\": thermal.select(\"ST_B10\"),\n",
        "    \"Soil_Moisture\": soil_moisture.select(\"sm_surface\"),\n",
        "    \"Segmented\": classified,\n",
        "    \"Predicted_Points\": ee.Image().paint(training_points, 1, 5)\n",
        "}\n",
        "\n",
        "# Optional: assign preferred matplotlib colormaps\n",
        "colormaps = {\n",
        "    \"World_Borders\": \"Greys\",\n",
        "    \"Study_Area\": \"Reds\",\n",
        "    \"Archaeology_Sites\": \"Blues\",\n",
        "    \"Sentinel2_Bands\": \"magma\",\n",
        "    \"Sentinel1_VV\": \"Greys\",\n",
        "    \"NDVI\": \"Greens\",\n",
        "    \"Elevation\": \"viridis\",\n",
        "    \"Thermal\": \"coolwarm\",\n",
        "    \"Soil_Moisture\": \"YlOrRd\",\n",
        "    \"Segmented\": \"tab10\",\n",
        "    \"Predicted_Points\": \"Greys\"\n",
        "}\n",
        "\n",
        "# Export and convert each layer\n",
        "for name, layer in raw_layers.items():\n",
        "    tif_path = os.path.join(output_dir, f\"{name}.tif\")\n",
        "    png_path = os.path.join(output_dir, f\"{name}.png\")\n",
        "\n",
        "    # Ensure it's visualized: raw numeric image â†’ RGB .visualize()\n",
        "    minmax = layer.reduceRegion(\n",
        "        reducer=ee.Reducer.minMax(),\n",
        "        geometry=region,\n",
        "        scale=scale,\n",
        "        maxPixels=1e9\n",
        "    ).getInfo()\n",
        "\n",
        "    band = layer.bandNames().getInfo()[0]\n",
        "    vmin, vmax = minmax[f\"{band}_min\"], minmax[f\"{band}_max\"]\n",
        "\n",
        "    vis_image = layer.visualize(\n",
        "        min=vmin,\n",
        "        max=vmax,\n",
        "        palette=[\"#000000\", \"#FFFFFF\"]  # placeholder; we'll recolor in PNG step\n",
        "    )\n",
        "\n",
        "    # Download as GeoTIFF\n",
        "    geemap.download_ee_image(\n",
        "        vis_image,\n",
        "        region=region,\n",
        "        filename=tif_path,\n",
        "        scale=scale,\n",
        "        crs=crs\n",
        "    )\n",
        "\n",
        "    # Convert GeoTIFF to normalized PNG with user-defined colormap\n",
        "    stretch_and_convert(tif_path, png_path, colormaps.get(name, \"viridis\"))\n",
        "    print(f\"âœ… Saved: {png_path}\")\n"
      ],
      "metadata": {
        "id": "Zqm69jnyjA6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "# Configure video parameters\n",
        "image_folder = 'exported_maps'\n",
        "video_name = 'Archaeology_Insights.mp4'\n",
        "frame_size = (800, 600)\n",
        "fps = 1  # seconds per frame\n",
        "\n",
        "image_files = sorted(glob(os.path.join(image_folder, \"*.png\")))\n",
        "\n",
        "# Initialize video writer\n",
        "video_writer = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, frame_size)\n",
        "\n",
        "for img_path in image_files:\n",
        "    img = cv2.imread(img_path)\n",
        "    resized = cv2.resize(img, frame_size)\n",
        "    video_writer.write(resized)\n",
        "\n",
        "video_writer.release()\n",
        "print(f\"ðŸŽ¬ Video created: {video_name}\")\n"
      ],
      "metadata": {
        "id": "DAAcRl-yTdkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read known archeological sites\n",
        "import csv\n",
        "import numpy as np\n",
        "with open(\"/content/Archaeological_Site_Features_2.csv\") as infile:\n",
        "    reader = csv.reader(infile, delimiter=\",\")\n",
        "    next(reader, None)\n",
        "    data3 =np.array(list(reader))"
      ],
      "metadata": {
        "id": "HXIes8X1vTJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the type of read parameters for the training of the model\n",
        "dtype1 = np.dtype([('B12', np.float32),('B11', np.float32),('B8', np.float32),('VV', np.float32),('NDVI', np.float32), ('Elevation', np.float32), ('SM', np.float32), ('ST', np.float32),('ET', np.float32),('Latitude', np.float32),('Longitude', np.float32) , ('Name', np.str_),('Class', np.int32)])\n",
        "arr2 = np.zeros((len(data3),)).astype(dtype1)"
      ],
      "metadata": {
        "id": "vOINVvLZv7IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xjUbTAZxANv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assign read data to another array of predefined type dtype1\n",
        "arr2['B11'] = data3[:, 0]\n",
        "arr2['B12'] = data3[:, 1]\n",
        "arr2['B8'] = data3[:, 2]\n",
        "arr2['ET'] = data3[:, 6]\n",
        "arr2['NDVI'] = data3[:, 5].astype(np.float32)\n",
        "arr2['ST'] = data3[:, 8].astype(np.float32)\n",
        "arr2['VV'] = data3[:, 3].astype(np.float32)\n",
        "arr2['Elevation'] = data3[:, 4]\n",
        "arr2['SM'] = data3[:, 7]\n",
        "arr2['Latitude'] = data3[:, 9]\n",
        "arr2['Longitude'] = data3[:, 10]\n",
        "arr2['Name'] = data3[:, 11]\n",
        "arr2['Class'] = data3[:, 12]"
      ],
      "metadata": {
        "id": "IH9rmZELz8ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, Input,Reshape\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# Define custom loss function (MSE + small L1 penalty)\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) + 0.01 * tf.reduce_sum(tf.abs(y_pred))\n",
        "def r2_score(y_true, y_pred):\n",
        "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return 1 - (SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
        "# Define feature columns (exclude Name, Latitude, Longitude)\n",
        "feature_columns = ['B12', 'B11', 'B8', 'VV',  'NDVI', 'Elevation', 'SM', 'ST', 'ET']\n",
        "\n",
        "X_structured = arr2[feature_columns]  # Extract features\n",
        "# Extract latitude & longitude properly from the structured array\n",
        "# Convert latitude & longitude to arcseconds (precision boost)\n",
        "arr2['Latitude'] = arr2['Latitude'] #* 3600\n",
        "arr2['Longitude'] = arr2['Longitude']# * 3600\n",
        "y = np.stack([arr2['Latitude'], arr2['Longitude']], axis=1).astype(np.float32)\n",
        "X = np.stack([X_structured[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "X_lstm1 = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "# Normalize features using MinMaxScaler\n",
        "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
        "X_scaled = scaler_X.fit_transform(X)  # Scale features\n",
        "\n",
        "scaler_y = MinMaxScaler()#feature_range=(0, 1))\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Reshape X to fit LSTM input format (samples, timesteps, features)\n",
        "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 1 timestep\n",
        "\n",
        "# Build LSTM model for predicting Latitude & Longitude of possible archeological sites\n",
        "\n",
        "input_layer = Input(shape=(1, len(feature_columns)))\n",
        "x = Bidirectional(LSTM(128, return_sequences=True))(input_layer)\n",
        "x = Dropout(0.3)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = LSTM(64, return_sequences=False)(x)  # Reduce complexity\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Dense(32, activation='relu')(x)\n",
        "output_layer = Dense(2, activation='linear')(x)  # Keep it linear for precise regression\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "# Compile model with MSE loss (regression task)\n",
        "#model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),loss=custom_loss, metrics= ['mse'])#['mse'])\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.Huber(delta=0.05),  # More robust loss function\n",
        "              metrics=['mse'])\n",
        "\n",
        "#model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.Huber(delta=1.0),metrics= ['mse'])#, metrics=['r2_score'])\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H-DjtTkH-0bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and plot the graphs of loss function and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "history = model.fit(X_lstm1, y, epochs=25, batch_size=16, validation_split=0.25, callbacks=[early_stopping])\n",
        "# Predict longitude & latitude for new site features\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy using MSE (lower is better)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mse'], label='Training mse')\n",
        "#lt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_mse'], label='Validation mse')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "#plt.ylabel('Mean Squared Error')\n",
        "plt.ylabel('mse')\n",
        "plt.title('Training vs Validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zxissEqD_bVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read synthetic data of possible archeological site with specific geographic chracteristics locations are unknown (to find latitude and longitude)\n",
        "import csv\n",
        "import numpy as np\n",
        "with open(\"/content/Samples_random_collection_2.csv\") as infile:\n",
        "    reader = csv.reader(infile, delimiter=\",\")\n",
        "    next(reader, None)\n",
        "    data4 =np.array(list(reader))"
      ],
      "metadata": {
        "id": "Av6DAK4HmRF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another tyep for a new array no location included\n",
        "dtype3 = np.dtype([('B11', np.float32),('B12', np.float32),('B8', np.float32), ('ET', np.float32),('NDVI', np.float32), ('ST', np.float32),('VV', np.float32),('Elevation', np.float32),('SM', np.float32) ])\n",
        "arr4 = np.zeros((len(data4),)).astype(dtype3)"
      ],
      "metadata": {
        "id": "A5dH4d3x9Ys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nPcInOGYlkTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assign the read attributes from samples file to arr4\n",
        "arr4['B11'] = data4[:, 0]\n",
        "arr4['B12'] = data4[:, 1]\n",
        "arr4['B8'] = data4[:, 2]\n",
        "arr4['ET'] = data4[:, 3]\n",
        "arr4['NDVI'] = data4[:, 4].astype(np.float32)\n",
        "arr4['ST'] = data4[:, 5].astype(np.float32)\n",
        "arr4['VV'] = data4[:, 6].astype(np.float32)\n",
        "arr4['Elevation'] = data4[:, 7]\n",
        "arr4['SM'] = data4[:, 8]"
      ],
      "metadata": {
        "id": "G2jyJo949h0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define feature columns\n",
        "feature_columns = ['B11', 'B12', 'B8','ET',  'NDVI',  'ST','VV', 'Elevation', 'SM']\n",
        "# Prepare input data\n",
        "new_data_for_scaling = np.stack([arr4[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "\n",
        "# Normalize the new data\n",
        "#new_scaled = scaler.transform(new_data_for_scaling)\n",
        "\n",
        "# Reshape for LSTM input format\n",
        "#new_lstm_ready = new_scaled.reshape((new_scaled.shape[0], 1, new_scaled.shape[1]))\n",
        "new_lstm_ready = new_data_for_scaling.reshape((new_data_for_scaling.shape[0], 1, new_data_for_scaling.shape[1]))\n",
        "# Make predictions (now expecting longitude & latitude)\n",
        "predicted_coords_scaled = model.predict(new_lstm_ready)\n",
        "#predicted_coords = scaler_y.inverse_transform(predicted_coords_scaled)\n",
        "# Convert predictions to a DataFrame\n",
        "results_df = pd.DataFrame(new_data_for_scaling, columns=feature_columns)  # Original input features\n",
        "\n",
        "# Add predicted longitude & latitude columns\n",
        "results_df['Predicted_Longitude'] = predicted_coords_scaled[:, 1]  # First column = longitude\n",
        "results_df['Predicted_Latitude'] = predicted_coords_scaled[:, 0]  # Second column = latitude\n",
        "\n",
        "# Save results to a CSV file\n",
        "results_df.to_csv('predicted_coordinates.csv', index=False)\n",
        "\n",
        "print(\"Predicted latitude and longitude saved to predicted_coordinates.csv\")\n"
      ],
      "metadata": {
        "id": "XVMxWUR4BUC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# True vs predicted coordinates\n",
        "true_lat = arr2[\"Latitude\"]\n",
        "true_lon = arr2[\"Longitude\"]\n",
        "pred_lat = results_df['Predicted_Latitude']#predicted_coords[:, 0]  # Latitude\n",
        "pred_lon =results_df['Predicted_Longitude']# predicted_coords[:, 1]  # Longitude\n",
        "print('true latitude',true_lat)\n",
        "print('true longitude',true_lon)\n",
        "print('predi latitude',pred_lat)\n",
        "print('predited longitude',pred_lon)\n",
        "# Calculate absolute errors for each point\n",
        "lat_errors = np.abs(true_lat - pred_lat)\n",
        "lon_errors = np.abs(true_lon- pred_lon)\n",
        "\n",
        "# Compute mean errors\n",
        "mean_lat_error = np.mean(lat_errors)\n",
        "mean_lon_error = np.mean(lon_errors)\n",
        "\n",
        "print(\"Individual Latitude Errors:\", lat_errors)\n",
        "print(\"Mean Latitude Error:\", mean_lat_error)\n",
        "print(\"Individual Longitude Errors:\", lon_errors)\n",
        "print(\"Mean Longitude Error:\", mean_lon_error)\n",
        "\n"
      ],
      "metadata": {
        "id": "J1vnPTtE7NKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot true locations\n",
        "plt.scatter(arr2[\"Longitude\"], arr2[\"Latitude\"], color='green', label='True Locations', marker='o')\n",
        "\n",
        "# Plot predicted locations\n",
        "#plt.scatter(pred_lon[:, 1], pred_lat[:, 0], color='red', label='Predicted Locations', marker='x')\n",
        "# Plot predicted locations\n",
        "plt.scatter(pred_lon.values, pred_lat.values, color='red', label='Predicted Locations', marker='x')\n",
        "# Connect true and predicted points to show displacement\n",
        "#for i in range(len(true_coords)):\n",
        "#    plt.plot([true_coords[i, 0], pred_coords[i, 0]], [true_coords[i, 1], pred_coords[i, 1]], color=\"gray\", linestyle=\"dashed\", alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"True vs. Predicted Locations of Archaeological Sites\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WeP1Hdti8Wjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}