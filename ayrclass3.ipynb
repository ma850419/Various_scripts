{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUvgUAkle0tujDpaq6PEAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Various_scripts/blob/main/ayrclass3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58kfKUEE2Chs"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='velvety-ring-328419')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "huGDsdu72P1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imageROI =ee.FeatureCollection('users/mohamadawadlebanon/borderutm36n')\n",
        "imageROI =ee.FeatureCollection('users/mohamadawadlebanon/Iranborder')\n",
        "#trainingPolys = ee.FeatureCollection('users/mohamadawadlebanon/fieldsamples_2019_36N')\n",
        "#evalPolys=ee.FeatureCollection('users/mohamadawadlebanon/fieldsamples_2019_36N')\n",
        "training = ee.FeatureCollection('users/mohamadawadlebanon/crop2')"
      ],
      "metadata": {
        "id": "Ya0oPJ3q2XIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_count = training.size().getInfo()\n",
        "print(f\"Number of records: {record_count}\")\n",
        "\n",
        "# Convert FeatureCollection to a list and print all features\n",
        "features = training.toList(record_count)\n",
        "for i in range(record_count):\n",
        "    feature = ee.Feature(features.get(i))\n",
        "    print(feature.getInfo())  # Prints full feature properties"
      ],
      "metadata": {
        "id": "Pk66Ts0P48Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "\n",
        "# Initialize Earth Engine\n",
        "\n",
        "# Use Sentinel-2 surface reflectance data.\n",
        "s2sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "\n",
        "# Cloud masking function for Sentinel-2.\n",
        "def maskS2sr(image):\n",
        "    qa = image.select('QA60')  # Sentinel-2 cloud mask band\n",
        "    mask = qa.eq(0)  # Keep only pixels where QA60 is clear\n",
        "    return image.updateMask(mask).divide(10000)  # Scale reflectance values\n",
        "\n",
        "# Clip function\n",
        "def clipimage(image):\n",
        "    return image.clip(imageROI)\n",
        "\n",
        "# Define Image ROI (Replace with a valid region)\n",
        "#imageROI = ee.Geometry.Rectangle([35, 30, 40, 35])  # Example ROI\n",
        "\n",
        "# Create a cloud-masked median composite for July 2019\n",
        "image = s2sr.filterDate('2024-05-01', '2024-05-31').map(clipimage).map(maskS2sr).median()\n",
        "print(image.getInfo())\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})  # RGB composite\n",
        "map = folium.Map(location=[28, 53], zoom_start=6)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='Sentinel-2 Composite',\n",
        ").add_to(map)\n",
        "\n",
        "'''mapid = image.getMapId({'bands': ['B11'], 'min': 0, 'max': 0.3})  # Thermal (SWIR Band)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='SWIR Band (B11)',\n",
        ").add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())'''\n",
        "\n",
        "# Display map\n",
        "map\n"
      ],
      "metadata": {
        "id": "dXKFrTEZ2gLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_np = geemap.ee_to_numpy(image, imageROI, bands=['B4', 'B3', 'B2'], scale=55)  # 10m resolution\n"
      ],
      "metadata": {
        "id": "foFp6gBzQjOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap\n",
        "import numpy as np\n",
        "\n",
        "# Extract image bands\n",
        "region = imageROI  # Define a bounding box (Iran region)\n",
        "\n",
        "# Convert to NumPy with specified region\n",
        "image_np = geemap.ee_to_numpy(image,imageROI, bands=['B4', 'B3', 'B2'], scale =60)\n",
        "\n",
        "'''training_image = training.reduceToImage(\n",
        "    properties=['class'],\n",
        "    reducer=ee.Reducer.first()\n",
        ")'''\n",
        "samples_image = training.reduceToImage(['class'], ee.Reducer.mode())  # Convert points to raster\n",
        "# Change 'class' to 'mode' to match the band name created by reduceToImage\n",
        "samples_np = geemap.ee_to_numpy(samples_image, region=region, bands=['mode'], scale =60)  # Extract pixels\n",
        "\n",
        "# Define region (Ensure it's bounded)\n",
        "\n",
        "# Convert to NumPy\n",
        "#samples_np = geemap.ee_to_numpy(training_image, region=region, scale=40)"
      ],
      "metadata": {
        "id": "zpx6sODM7b1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(samples_np))"
      ],
      "metadata": {
        "id": "UnJXKSU76pq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_np = np.expand_dims(image_np, axis=-1)  # Converts to (height, width, 1)\n",
        "image_np = np.expand_dims(image_np, axis=0)  # Becomes (1, 1439, 1825, 3)\n",
        "samples_np = np.expand_dims(samples_np, axis=0)  # Becomes (1, 1439, 1825, 1)\n",
        "\n",
        "print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "print(samples_np.shape )"
      ],
      "metadata": {
        "id": "a5vrt3i9A_q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_height = image_np.shape[1] - (image_np.shape[1] % 2)  # Ensure divisible by 2\n",
        "new_width = image_np.shape[2] - (image_np.shape[2] % 2)\n",
        "\n",
        "image_np = image_np[:, :new_height, :new_width, :]  # Trim image size\n",
        "samples_np = samples_np[:, :new_height, :new_width, :]  # Match target size\n",
        "print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "print(samples_np.shape )"
      ],
      "metadata": {
        "id": "qH9GgwDubKud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique values in samples_np before encoding:\", np.unique(samples_np))\n"
      ],
      "metadata": {
        "id": "35m0J9VHt9aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_np = np.expand_dims(image_np, axis=-1)  # Converts to (height, width, 1)\n",
        "#image_np = np.expand_dims(image_np, axis=0)  # Becomes (1, 1439, 1825, 3)\n",
        "#samples_np = np.expand_dims(samples_np, axis=0)  # Becomes (1, 1439, 1825, 1)\n",
        "\n",
        "#print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "#print(samples_np.shape )\n",
        "\n",
        "# One-hot encode the target labels\n",
        "# Subtract 1 from samples_np to make class indices 0-based\n",
        "\n",
        "# One-hot encode the target labels\n",
        "samples_one_hot = tf.keras.utils.to_categorical(samples_np, num_classes=21)\n",
        "\n",
        "# Remove the extra dimension added by to_categorical if necessary\n",
        "if samples_one_hot.ndim == 5 and samples_one_hot.shape[-2] == 1:\n",
        "    samples_one_hot = np.squeeze(samples_one_hot, axis=-2)\n",
        "\n",
        "# Convert to integer format\n",
        "samples_one_hot = samples_one_hot.astype(int)\n",
        "\n",
        "print(\"Updated shape:\", samples_one_hot)\n",
        "print(\"Unique values:\", np.unique(samples_one_hot))  # Should contain only 0s and 1s"
      ],
      "metadata": {
        "id": "9ssT57FW1RC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure samples_np has correct dimensions\n",
        "print(\"samples_np shape:\", samples_np.shape)  # Expected: (1, 1438, 1824)\n",
        "\n",
        "# Check samples_one_hot dimensions\n",
        "print(\"samples_one_hot shape:\", samples_one_hot.shape)  # Expected: (1, 1438, 1824, 21)\n",
        "\n",
        "# Create mask that matches the exact shape of samples_one_hot\n",
        "mask = np.repeat(samples_np == 0, repeats=samples_one_hot.shape[-1], axis=-1)  # Shape: (1, 1438, 1824, 21)\n",
        "\n",
        "# Apply mask to remove No Data pixels\n",
        "samples_one_hot[mask] = 0\n",
        "\n",
        "# Verify changes\n",
        "print(\"Updated shape:\", samples_one_hot.shape)\n",
        "print(\"Unique values:\", np.unique(samples_one_hot))  # Should contain only 0s and 1s\n"
      ],
      "metadata": {
        "id": "9uDvPOoOgtSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sum pixel occurrences for each class (excluding spatial dimensions)\n",
        "class_distribution = np.sum(samples_one_hot, axis=(0, 1))  # Shape: (21,)\n",
        "\n",
        "\n",
        "# Remove No Data class (class 0)\n",
        "class_distribution_filtered = class_distribution[1:]  # Excludes class 0\n",
        "print(\"Filtered class distribution:\", class_distribution_filtered)\n",
        "\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, 21), class_distribution_filtered, color='blue', alpha=0.7)\n",
        "plt.xlabel(\"Class Labels (excluding No Data)\")\n",
        "plt.ylabel(\"Frequency of '1' Values\")\n",
        "plt.title(\"Class Distribution in samples_one_hot (No Data Removed)\")\n",
        "plt.xticks(range(1, 21))\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# Print detailed class counts\n",
        "for cls, count in enumerate(class_distribution_filtered, start=1):  # Start from class 1\n",
        "    print(f\"Class {cls}: {count} pixels\")\n"
      ],
      "metadata": {
        "id": "NoBJ_n6miQ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_np = np.expand_dims(image_np, axis=0)  # Becomes (1, 1439, 1825, 3)\n",
        "samples_one_hot = np.expand_dims(samples_one_hot, axis=0)  # Becomes (1, 1439, 1825, 1)\n",
        "\n",
        "print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "print(samples_one_hot.shape )"
      ],
      "metadata": {
        "id": "iQTVcFp1jVdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_height = image_np.shape[1] - (image_np.shape[1] % 2)  # Ensure divisible by 2\n",
        "new_width = image_np.shape[2] - (image_np.shape[2] % 2)\n",
        "\n",
        "image_np = image_np[:, :new_height, :new_width, :]  # Trim image size\n",
        "samples_one_hot = samples_one_hot[:, :new_height, :new_width, :]  # Match target size\n",
        "print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "print(samples_one_hot.shape )"
      ],
      "metadata": {
        "id": "c0FA632S2dyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(samples_one_hot)"
      ],
      "metadata": {
        "id": "gV3DKEMwC-PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow import keras\n",
        "\n",
        "# Define U-Net Model for 20-class classification\n",
        "model = tf.keras.Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(image_np.shape[1], image_np.shape[2], image_np.shape[3])),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    UpSampling2D((2, 2)),\n",
        "    Conv2D(20, (1, 1), activation='softmax', padding='same')  # Adjusted for multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model (using GEE sample data)\n",
        "model.fit(image_np, samples_one_hot[:,:,:,1:21], epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "cLXuoq3_9r85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, BatchNormalization\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # Encoder\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(image_np.shape[1], image_np.shape[2], image_np.shape[3])),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Decoder\n",
        "    UpSampling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    UpSampling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    UpSampling2D((2, 2)),\n",
        "    Conv2D(20, (1, 1), activation='softmax', padding='same')  # Multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(image_np, samples_one_hot[:,:,:,1:21], epochs=10, batch_size=16)#, class_weight=class_weights)\n"
      ],
      "metadata": {
        "id": "R4avp7hYxO3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_np.shape)\n",
        "#print(image_np)  # Inspect sample values\n",
        "\n",
        "print(samples_one_hot.shape )\n",
        "print(\"Image shape:\", image_np.shape)\n",
        "print(\"Labels shape:\", samples_one_hot[:,:,:,1:21].shape)"
      ],
      "metadata": {
        "id": "cRDvRIiczuwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(image_np)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "87YyXyfazgHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the band index with the highest value per pixel\n",
        "class_predictions = np.argmax(predictions, axis=-1)  # Shape: (1438, 1824)\n",
        "\n",
        "# Adjust indices to match bands 1-20 (instead of 0-19)\n",
        "class_predictions += 1\n",
        "\n",
        "# Verify results\n",
        "print(\"Updated shape:\", class_predictions.shape)  # Should be (1438, 1824)\n",
        "print(\"Unique classes:\", np.unique(class_predictions))  # Should contain values [1, 2, ..., 20]\n"
      ],
      "metadata": {
        "id": "6e6Z_2R-u-b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_params = {\n",
        "    'min': 1,\n",
        "    'max': 20,\n",
        "    'palette': [\n",
        "        'red', 'yellow', 'green', 'blue', 'purple', 'orange', 'brown', 'cyan', 'pink',\n",
        "        'magenta', 'lime', 'gray', 'black', 'white', 'gold', 'navy', 'teal', 'maroon',\n",
        "        'turquoise', 'violet'\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "wEfsBcG78_tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_image = predicted_image.setDefaultProjection('EPSG:4326', None, 30)  # WGS84 with 30m scale\n",
        "\n"
      ],
      "metadata": {
        "id": "XGlTU84i97gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions_scaled.shape)\n",
        "  # Ensure it prints valid geometric coordinates\n",
        "pixels = ee.Array(predictions_scaled.tolist())  # Convert NumPy array to EE array\n",
        "predicted_image = ee.Image(pixels).clip(imageROI)  # Clip to the target region\n"
      ],
      "metadata": {
        "id": "O6VyGAr69_F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_image.projection().getInfo())  # Confirms raster properties\n"
      ],
      "metadata": {
        "id": "o4Ey2ZDxG9Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "kzcfweR1_g7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map.addLayer(predicted_image, {'min': 1, 'max': 20, 'palette': ['red', 'blue', 'green']}, \"Predicted Image\")\n"
      ],
      "metadata": {
        "id": "mu1LHbz4-Uth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.addLayer(predictions_scaled, viz_params, \"U-Net Classified Image\")\n",
        "Map.centerObject(region, 10)  # Adjust zoom level\n",
        "Map\n"
      ],
      "metadata": {
        "id": "WShTnYTb9BEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_image_ee = geemap.numpy_to_ee(predictions_scaled, imageROI)\n",
        "\n",
        "# Set the projection and scale explicitly\n",
        "predicted_image_ee = predicted_image_ee.setDefaultProjection(\n",
        "    crs='EPSG:4326',  # Specify a common projection\n",
        "    scale=60          # Use the same scale as when converting to numpy\n",
        ")\n",
        "\n",
        "\n",
        "Map = geemap.Map()\n",
        "# Add the Earth Engine image to the map\n",
        "Map.addLayer(predicted_image_ee, {}, \"U-Net Classified Image\")\n",
        "Map.centerObject(imageROI.geometry(), 10)  # Adjust zoom level and use the region's geometry\n",
        "Map"
      ],
      "metadata": {
        "id": "WYhdywiIJg3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}