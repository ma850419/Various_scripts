{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Various_scripts/blob/main/from_gee_to_colab_archeology_6june2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJHo6-Sy4sLQ"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='velvety-ring-328419')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install geemap if not already installed\n",
        "!pip install geemap\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwoJA0eSaHCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# based on similar date\n",
        "import geemap\n",
        "\n",
        "# Define the region of interest (Eastern Brazil)\n",
        "mask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_na', 'Brazil'))\n",
        "brazil_geometry = mask.geometry()\n",
        "\n",
        "# Clip to Eastern Brazil using longitude filtering\n",
        "#east_brazil = brazil_geometry.intersection(ee.Geometry.Rectangle([-40, -5, -30, -30]))  # Adjust as needed\n",
        "southeast_brazil = brazil_geometry.intersection(ee.Geometry.Rectangle([-55, -25, -40, -35]))  # Adjust as needed\n",
        "# Load archaeological points\n",
        "archaeology_points = ee.FeatureCollection('users/mohamadawadlebanon/Archeologicalsites')\n",
        "\n",
        "# Define date range\n",
        "start_date = '2024-05-01'\n",
        "end_date = '2024-05-31'\n",
        "def classify_image(date):\n",
        "    date = ee.Date(date)\n",
        "\n",
        "    sentinel2 = mosaiced_sentinel2.filter(ee.Filter.eq(\"date\", date.format(\"YYYY-MM-dd\"))).first()\n",
        "    sentinel1 = mosaiced_sentinel1.filter(ee.Filter.eq(\"date\", date.format(\"YYYY-MM-dd\"))).first()\n",
        "\n",
        "    combined = sentinel2.addBands(sentinel1)\n",
        "\n",
        "    classified = ee.Algorithms.If(\n",
        "        archaeology_points.size().gt(0),\n",
        "        combined.classify(\n",
        "            ee.Classifier.smileRandomForest(10).train(\n",
        "                combined.sampleRegions(collection=archaeology_points, properties=['class'], scale=30),\n",
        "                'class'\n",
        "            )\n",
        "        ),\n",
        "        ee.Image.constant(-9999).rename(\"classification\")  # Placeholder if no training points\n",
        "    )\n",
        "\n",
        "    return ee.Image(classified).set(\"date\", date.format(\"YYYY-MM-dd\"))\n",
        "### Step 1: Mosaic Sentinel-1 images per date ###\n",
        "def mosaic_sentinel1(date):\n",
        "    date = ee.Date(date)\n",
        "    sentinel1_images = ee.ImageCollection(\"COPERNICUS/S1_GRD\").filterBounds(southeast_brazil) \\\n",
        "        .filterDate(date, date.advance(1, 'day')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .select(\"VV\")\n",
        "\n",
        "    return sentinel1_images.median().set(\"date\", date.format(\"YYYY-MM-dd\"))\n",
        "\n",
        "### Step 2: Mosaic Sentinel-2 images per date ###\n",
        "def mosaic_sentinel2(date):\n",
        "    date = ee.Date(date)\n",
        "    sentinel2_images = ee.ImageCollection(\"COPERNICUS/S2\").filterBounds(southeast_brazil) \\\n",
        "        .filterDate(date, date.advance(1, 'day')) \\\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \\\n",
        "        .select([\"B8\", \"B11\", \"B12\"])\n",
        "\n",
        "    return sentinel2_images.median().set(\"date\", date.format(\"YYYY-MM-dd\"))\n",
        "\n",
        "# Extract available dates\n",
        "sentinel1_dates = ee.ImageCollection(\"COPERNICUS/S1_GRD\").filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "    .aggregate_array('system:time_start').map(ee.Date)\n",
        "\n",
        "sentinel2_dates = ee.ImageCollection(\"COPERNICUS/S2\").filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) \\\n",
        "    .aggregate_array('system:time_start').map(ee.Date)\n",
        "\n",
        "# Find matching dates between Sentinel-1 and Sentinel-2\n",
        "formatted_sentinel1_dates = sentinel1_dates.map(lambda date: ee.Date(date).format(\"YYYY-MM-dd\"))\n",
        "formatted_sentinel2_dates = sentinel2_dates.map(lambda date: ee.Date(date).format(\"YYYY-MM-dd\"))\n",
        "\n",
        "common_dates = formatted_sentinel1_dates.filter(ee.Filter.inList('item', formatted_sentinel2_dates))\n",
        "\n",
        "# Create mosaiced collections\n",
        "mosaiced_sentinel1 = ee.ImageCollection(common_dates.map(mosaic_sentinel1))\n",
        "mosaiced_sentinel2 = ee.ImageCollection(common_dates.map(mosaic_sentinel2))\n",
        "\n",
        "# Create Map\n",
        "m = geemap.Map(center=[-20, -40], zoom=5)  # Centered around Eastern Brazil\n",
        "\n",
        "# Display Sentinel-1 mosaiced images\n",
        "list_s1 = mosaiced_sentinel1.toList(mosaiced_sentinel1.size())\n",
        "for i in range(list_s1.size().getInfo()):\n",
        "    img = ee.Image(list_s1.get(i))\n",
        "    date_label = img.get(\"date\").getInfo()\n",
        "    vis_params_s1 = {\"bands\": [\"VV\"], \"min\": -20, \"max\": 0, \"gamma\": 1.4}\n",
        "    m.addLayer(img, vis_params_s1, f\"Sentinel-1 Mosaiced ({date_label})\")\n",
        "\n",
        "# Display Sentinel-2 mosaiced images\n",
        "list_s2 = mosaiced_sentinel2.toList(mosaiced_sentinel2.size())\n",
        "for i in range(list_s2.size().getInfo()):\n",
        "    img = ee.Image(list_s2.get(i))\n",
        "    date_label = img.get(\"date\").getInfo()\n",
        "    vis_params_s2 = {\"bands\": [\"B12\", \"B11\", \"B8\"], \"min\": 0, \"max\": 3000, \"gamma\": 1.4}\n",
        "    m.addLayer(img, vis_params_s2, f\"Sentinel-2 Mosaiced ({date_label})\")\n",
        "'''classified_images = ee.ImageCollection(common_dates.map(classify_image))\n",
        "print(\"Number of classified images:\", classified_images.size().getInfo())\n",
        "# Display the map\n",
        "# Display Classified Images\n",
        "list_classified = classified_images.toList(classified_images.size())\n",
        "for i in range(list_classified.size().getInfo()):\n",
        "    img = ee.Image(list_classified.get(i))\n",
        "    date_label = img.get(\"date\").getInfo()\n",
        "    vis_params_classified = {\n",
        "        \"bands\": [\"classification\"],\n",
        "        \"min\": 0,\n",
        "        \"max\": 1,\n",
        "        \"palette\": [\"red\", \"yellow\", \"green\"]\n",
        "    }\n",
        "    m.addLayer(img, vis_params_classified, f\"Classified Archaeological Predictions ({date_label})\")'''\n",
        "# Add the archaeology points layer\n",
        "archaeology_vis = {\n",
        "    'color': 'blue',\n",
        "    'pointRadius': 5\n",
        "}\n",
        "m.addLayer(archaeology_points, archaeology_vis, 'Archaeology Points')\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "Szg4yHn7m-af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Kxc89j1r8vhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/ee-packages-py-main/"
      ],
      "metadata": {
        "id": "j7Gl5orGCMig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on similar location\n",
        "import geemap\n",
        "\n",
        "# Define region (Brazil, Southeastern Amazon)\n",
        "mask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_na', 'Brazil'))\n",
        "brazil_geometry = mask.geometry()\n",
        "southeast_brazil = brazil_geometry.intersection(ee.Geometry.Rectangle([-55, -25, -40, -35]))\n",
        "\n",
        "# Load archaeological points\n",
        "archaeology_points = ee.FeatureCollection(\"users/mohamadawadlebanon/Archeologicalsites\")\n",
        "\n",
        "# Define date range\n",
        "start_date = \"2024-05-01\"\n",
        "end_date = \"2024-05-31\"\n",
        "def apply_scale_and_offset(image):\n",
        "    return image.select(\"ST_B10\").multiply(0.00341802).add(149.0)\n",
        "### **Step 1: Elevation Data (ASTER GDEM)**\n",
        "elevation = ee.Image(\"projects/sat-io/open-datasets/ASTER/GDEM\").clip(southeast_brazil)\n",
        "#elevation = elevation.rename(\"Elevation\")\n",
        "elevation = elevation.rename(\"elevation\")\n",
        "### **Step 2: Sentinel-2 EVI**\n",
        "sentinel2_ndvi = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B4\"]) \\\n",
        "    .map(lambda img: img.expression(\n",
        "        \"(B8 - B4) / (B8 + B4)\",  # EVI formula\n",
        "        {\"B8\": img.select(\"B8\"), \"B4\": img.select(\"B4\")}\n",
        "    )).median().clip(southeast_brazil)\n",
        "sentinel2_ndvi = sentinel2_ndvi.rename(\"NDVI\")\n",
        "\n",
        "### **Step 3: Sentinel-1 Radar**\n",
        "sentinel1_images = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\")) \\\n",
        "    .filter(ee.Filter.eq(\"instrumentMode\", \"IW\")) \\\n",
        "    .select(\"VV\")\n",
        "\n",
        "mosaiced_sentinel1 = sentinel1_images.median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 4: Sentinel-2 Optical Mosaic**\n",
        "sentinel2_images = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B11\", \"B12\"])\n",
        "\n",
        "mosaiced_sentinel2 = sentinel2_images.median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 5: MODIS Evapotranspiration (ET)**\n",
        "modis_et = ee.ImageCollection(\"MODIS/061/MOD16A2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select(\"ET\") \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "'''### **Step 6: Penman-Monteith-Leuning Evapotranspiration (PML-V2)**\n",
        "pml_et = ee.ImageCollection(\"CAS/IGSNRR/PML/V2_v018\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select(\"ET\") \\\n",
        "    .median().clip(southeast_brazil)'''\n",
        "\n",
        "### **Step 7: Soil Moisture (NASA SMAP)**\n",
        "soil_moisture = ee.ImageCollection('NASA/SMAP/SPL4SMGP/007') \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select('sm_surface') \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 8: Thermal Infrared (Landsat 8-9)**\n",
        "thermal = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .map(apply_scale_and_offset) \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 9: Combine Layers for Classification**\n",
        "combined = mosaiced_sentinel2.addBands(mosaiced_sentinel1) \\\n",
        "    .addBands(elevation).addBands(sentinel2_ndvi).addBands(modis_et) \\\n",
        "    .addBands(soil_moisture).addBands(thermal)\n",
        "\n",
        "# Sample feature values at archaeology site locations\n",
        "sampled_values = combined.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 10,  # Adjust scale depending on resolution needs\n",
        "    #\"properties\": [\"site_id\"],  # Add relevant properties\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "# Convert sampled values to FeatureCollection table\n",
        "def safe_set_coordinates(feature):\n",
        "    return feature.set({\n",
        "        \"Longitude\": feature.get(\"lon\"),\n",
        "        \"Latitude\": feature.get(\"lat\")\n",
        "    })\n",
        "\n",
        "#table = sampled_values.map(safe_set_coordinates)\n",
        "\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=sampled_values, #table,\n",
        "    description=\"Archaeological_Site_Features\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task.start()  # Start export task\n",
        "\n",
        "### **Step 10: Apply K-Means Clustering**\n",
        "num_classes = 10\n",
        "'''training_points = combined.sample(**{\n",
        "    \"region\": southeast_brazil,\n",
        "    \"scale\": 10,\n",
        "    \"numPixels\": 500,\n",
        "    \"seed\": 42\n",
        "})'''\n",
        "training_points = combined.sample(\n",
        "    region=southeast_brazil,\n",
        "    # Default (False) is no geometries in the output.\n",
        "    # When set to True, each feature has a Point geometry at the center of the\n",
        "    # image pixel.\n",
        "    geometries=True,\n",
        "    numPixels = 500,\n",
        "    # The scale is not specified, so the resolution of the image will be used,\n",
        "    # and there is a feature for every pixel. If we give a scale parameter, the\n",
        "    # image will be resampled and there will be more or fewer features.\n",
        "    #\n",
        "    scale=10,\n",
        ")\n",
        "# Extract longitude and latitude from geometry\n",
        "'''training_points = training_points.map(lambda feature:\n",
        "                     feature.set({\n",
        "                         \"Longitude\": feature.geometry().coordinates().get(0),\n",
        "                         \"Latitude\": feature.geometry().coordinates().get(1)\n",
        "                     })\n",
        "\n",
        "    )'''\n",
        "#training_points = training_points.filter(ee.Filter.notNull(['geometry']))\n",
        "print(training_points.first().getInfo())\n",
        "task1 = ee.batch.Export.table.toDrive(\n",
        "    collection=training_points,\n",
        "    description=\"Samples_random_collection\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task1.start()\n",
        "clusterer = ee.Clusterer.wekaKMeans(num_classes).train(training_points)\n",
        "classified = combined.cluster(clusterer)\n",
        "\n",
        "### **Step 11: Validate Using Archaeological Sites**\n",
        "validation = classified.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 10,\n",
        "    \"properties\": [\"class\"],\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "\n",
        "### **Step 12: Visualization Parameters**\n",
        "vis_params_elevation = {\"bands\": [\"elevation\"], \"min\": 0, \"max\": 3000, \"palette\": [\"black\", \"white\"]}\n",
        "vis_params_ndvi = {\"bands\": [\"NDVI\"],\"min\": -0.6, \"max\": 0.6, \"palette\": [\"brown\", \"green\"]}\n",
        "vis_params_s1 = {\"bands\": [\"VV\"], \"min\": -20, \"max\": 0, \"gamma\": 1.4}\n",
        "vis_params_s2 = {\"bands\": [\"B12\", \"B11\", \"B8\"], \"min\": 0, \"max\": 3000, \"gamma\": 1.4}\n",
        "vis_params_modis_et = {\"bands\": [\"ET\"], \"min\": 0, \"max\": 300, \"palette\": [\"yellow\", \"green\", \"blue\"]}\n",
        "#vis_params_pml_et = {\"min\": 0, \"max\": 5, \"palette\": [\"orange\", \"red\", \"purple\"]}\n",
        "vis_params_soil = {\"bands\": [\"sm_surface\"], \"min\": 0, \"max\": 0.9, \"palette\": [\"red\", \"orange\", \"yellow\", \"green\"]}\n",
        "vis_params_thermal = {\"bands\": [\"ST_B10\"], \"min\": 270, \"max\": 320, \"palette\": [\"blue\", \"yellow\", \"red\"]}\n",
        "vis_params_classified = {\n",
        "    \"min\": 0,\n",
        "    \"max\": num_classes - 1,\n",
        "    \"palette\": [\"blue\", \"green\", \"yellow\", \"red\", \"purple\", \"orange\", \"brown\", \"cyan\", \"pink\", \"gray\"]\n",
        "}\n",
        "archaeology_vis = {\"color\": \"blue\", \"pointRadius\": 5}\n",
        "sample_vis = {\"color\": \"black\", \"pointRadius\": 4}\n",
        "### **Step 13: Create & Display Map**\n",
        "m = geemap.Map(center=[-3, -60], zoom=6)\n",
        "\n",
        "m.addLayer(elevation, vis_params_elevation, \"ASTER GDEM v3 Elevation\")\n",
        "m.addLayer(sentinel2_ndvi, vis_params_ndvi, \"High-Resolution NDVI (Sentinel-2)\")\n",
        "m.addLayer(mosaiced_sentinel1, vis_params_s1, \"Sentinel-1 Mosaic (Radar)\")\n",
        "m.addLayer(mosaiced_sentinel2, vis_params_s2, \"Sentinel-2 Mosaic (Optical)\")\n",
        "m.addLayer(modis_et, vis_params_modis_et, \"MODIS Evapotranspiration\")\n",
        "#m.addLayer(pml_et, vis_params_pml_et, \"PML Evapotranspiration\")\n",
        "m.addLayer(soil_moisture, vis_params_soil, \"Soil Moisture (SMAP)\")\n",
        "m.addLayer(thermal, vis_params_thermal, \"Thermal Infrared (Landsat)\")\n",
        "m.addLayer(classified, vis_params_classified, \"Classified Image (K-Means)\")\n",
        "m.addLayer(archaeology_points, archaeology_vis, \"Archaeology Points\")\n",
        "m.addLayer(training_points,sample_vis, \"sample Points\")\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "QPv8fZWNPhB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_points.size().getInfo())  # Ensure points were actually sampled\n",
        "\n"
      ],
      "metadata": {
        "id": "M7XgbKBs8O2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on similar location - modified one to include weights\n",
        "import geemap\n",
        "\n",
        "# Define region (Brazil, Southeastern Amazon)\n",
        "mask = ee.FeatureCollection('USDOS/LSIB_SIMPLE/2017').filter(ee.Filter.eq('country_na', 'Brazil'))\n",
        "brazil_geometry = mask.geometry()\n",
        "southeast_brazil = brazil_geometry.intersection(ee.Geometry.Rectangle([-55, -25, -40, -35]))\n",
        "weights = {\n",
        "    \"B12\": 0.3,\n",
        "    \"B11\": 0.25,\n",
        "    \"B8\": 0.2,\n",
        "    \"VV\": 0.15,\n",
        "    #\"NDVI\": 0.1,\n",
        "    #\"Elevation\": 0.2,\n",
        "    #\"SoilMoisture\": 0.15,\n",
        "    #\"Thermal\": 0.2,\n",
        "    #\"Evapotranspiration\": 0.1\n",
        "}\n",
        "def normalize_band(image, band):\n",
        "    band_min = image.select(band).reduceRegion(\n",
        "        reducer=ee.Reducer.min(), geometry=image.geometry(), scale=30, bestEffort=True\n",
        "    ).get(band)\n",
        "\n",
        "    band_max = image.select(band).reduceRegion(\n",
        "        reducer=ee.Reducer.max(), geometry=image.geometry(), scale=30, bestEffort=True\n",
        "    ).get(band)\n",
        "\n",
        "    normalized_band = image.select(band).subtract(ee.Number(band_min)).divide(ee.Number(band_max).subtract(ee.Number(band_min)))\n",
        "    return normalized_band.rename(band + \"_norm\")\n",
        "# Load archaeological points\n",
        "archaeology_points = ee.FeatureCollection(\"users/mohamadawadlebanon/Archeologicalsites\")\n",
        "\n",
        "# Define date range\n",
        "start_date = \"2024-05-01\"\n",
        "end_date = \"2024-05-31\"\n",
        "def apply_scale_and_offset(image):\n",
        "    return image.select(\"ST_B10\").multiply(0.00341802).add(149.0)\n",
        "### **Step 1: Elevation Data (ASTER GDEM)**\n",
        "elevation = ee.Image(\"projects/sat-io/open-datasets/ASTER/GDEM\").clip(southeast_brazil)\n",
        "#elevation = elevation.rename(\"Elevation\")\n",
        "elevation = elevation.rename(\"elevation\")\n",
        "### **Step 2: Sentinel-2 EVI**\n",
        "sentinel2_ndvi = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B4\"]) \\\n",
        "    .map(lambda img: img.expression(\n",
        "        \"(B8 - B4) / (B8 + B4)\",  # EVI formula\n",
        "        {\"B8\": img.select(\"B8\"), \"B4\": img.select(\"B4\")}\n",
        "    )).median().clip(southeast_brazil)\n",
        "sentinel2_ndvi = sentinel2_ndvi.rename(\"NDVI\")\n",
        "\n",
        "### **Step 3: Sentinel-1 Radar**\n",
        "sentinel1_images = ee.ImageCollection(\"COPERNICUS/S1_GRD\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.listContains(\"transmitterReceiverPolarisation\", \"VV\")) \\\n",
        "    .filter(ee.Filter.eq(\"instrumentMode\", \"IW\")) \\\n",
        "    .select(\"VV\")\n",
        "\n",
        "mosaiced_sentinel1 = sentinel1_images.median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 4: Sentinel-2 Optical Mosaic**\n",
        "sentinel2_images = ee.ImageCollection(\"COPERNICUS/S2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 10)) \\\n",
        "    .select([\"B8\", \"B11\", \"B12\"])\n",
        "\n",
        "mosaiced_sentinel2 = sentinel2_images.median().clip(southeast_brazil)\n",
        "'''normalized_sentinel2 = ee.Image.cat(\n",
        "    normalize_band(mosaiced_sentinel2, \"B12\"),\n",
        "    normalize_band(mosaiced_sentinel2, \"B11\"),\n",
        "    normalize_band(mosaiced_sentinel2, \"B8\")\n",
        ")'''\n",
        "\n",
        "### **Step 5: MODIS Evapotranspiration (ET)**\n",
        "modis_et = ee.ImageCollection(\"MODIS/061/MOD16A2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select(\"ET\") \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "'''### **Step 6: Penman-Monteith-Leuning Evapotranspiration (PML-V2)**\n",
        "pml_et = ee.ImageCollection(\"CAS/IGSNRR/PML/V2_v018\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select(\"ET\") \\\n",
        "    .median().clip(southeast_brazil)'''\n",
        "\n",
        "### **Step 7: Soil Moisture (NASA SMAP)**\n",
        "soil_moisture = ee.ImageCollection('NASA/SMAP/SPL4SMGP/007') \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .select('sm_surface') \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "### **Step 8: Thermal Infrared (Landsat 8-9)**\n",
        "thermal = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n",
        "    .filterBounds(southeast_brazil) \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .map(apply_scale_and_offset) \\\n",
        "    .median().clip(southeast_brazil)\n",
        "\n",
        "\n",
        "# Normalize all bands\n",
        "#normalized_sentinel2 = mosaiced_sentinel2.select([\"B12\", \"B11\", \"B8\"]).map(lambda img: normalize_band(mosaiced_sentinel2, img))\n",
        "normalized_sentinel1 = normalize_band(mosaiced_sentinel1, \"VV\")\n",
        "normalized_ndvi = normalize_band(sentinel2_ndvi, \"NDVI\")\n",
        "normalized_elevation = normalize_band(elevation, \"elevation\")\n",
        "normalized_soil_moisture = normalize_band(soil_moisture, \"sm_surface\")\n",
        "normalized_thermal = normalize_band(thermal, \"ST_B10\")\n",
        "normalized_et = normalize_band(modis_et, \"ET\")\n",
        "\n",
        "### **Step 9: Combine Layers for Classification**\n",
        "'''weighted_combined = (normalized_sentinel2.select(\"B12_norm\").multiply(weights[\"B12\"])\n",
        "    .add(normalized_sentinel2.select(\"B11_norm\").multiply(weights[\"B11\"]))\n",
        "    .add(normalized_sentinel2.select(\"B8_norm\").multiply(weights[\"B8\"]))\n",
        "    .add(normalized_sentinel1.multiply(weights[\"VV\"]))\n",
        "    .add(normalized_ndvi.multiply(weights[\"NDVI\"]))\n",
        "    .add(normalized_elevation.multiply(weights[\"Elevation\"]))\n",
        "    .add(normalized_soil_moisture.multiply(weights[\"SoilMoisture\"]))\n",
        "    .add(normalized_thermal.multiply(weights[\"Thermal\"]))\n",
        "    .add(normalized_et.multiply(weights[\"Evapotranspiration\"])\n",
        "    .rename(\"Weighted_Combined\"))'''\n",
        "#combined = normalized_sentinel2.select(\"B12_norm\").multiply(weights[\"B12\"]).addBands(normalized_sentinel2.select(\"B11_norm\").multiply(weights[\"B11\"])).addBands(normalized_sentinel2.select(\"B8_norm\").multiply(weights[\"B8\"])).addBands(normalized_sentinel1.multiply(weights[\"VV\"])).addBands(normalized_ndvi.multiply(weights[\"NDVI\"])).addBands(normalized_elevation.multiply(weights[\"Elevation\"])).addBands(normalized_soil_moisture.multiply(weights[\"SoilMoisture\"])).addBands(normalized_thermal.multiply(weights[\"Thermal\"])).addBands(normalized_et.multiply(weights[\"Evapotranspiration\"]))\n",
        "#combined = mosaiced_sentinel2.select(\"B12\").multiply(weights[\"B12\"]).addBands(mosaiced_sentinel2.select(\"B11\").multiply(weights[\"B11\"])).addBands(mosaiced_sentinel2.select(\"B8\").multiply(weights[\"B8\"])).addBands(mosaiced_sentinel1.multiply(weights[\"VV\"])).addBands(sentinel2_ndvi.multiply(weights[\"NDVI\"])).addBands(elevation.multiply(weights[\"Elevation\"])).addBands(soil_moisture.multiply(weights[\"SoilMoisture\"])).addBands(thermal.multiply(weights[\"Thermal\"])).addBands(modis_et.multiply(weights[\"Evapotranspiration\"]))\n",
        "#combined = (sentinel2_ndvi.multiply(weights[\"NDVI\"])).addBands(elevation.multiply(weights[\"Elevation\"])).addBands(soil_moisture.multiply(weights[\"SoilMoisture\"])).addBands(thermal.multiply(weights[\"Thermal\"])).addBands(modis_et.multiply(weights[\"Evapotranspiration\"]))\n",
        "combined = mosaiced_sentinel2.select(\"B12\").multiply(weights[\"B12\"]).addBands(mosaiced_sentinel2.select(\"B11\").multiply(weights[\"B11\"])).addBands(mosaiced_sentinel2.select(\"B8\").multiply(weights[\"B8\"])).addBands(mosaiced_sentinel1.multiply(weights[\"VV\"]))\n",
        "'''combined1 = mosaiced_sentinel2.addBands(mosaiced_sentinel1) \\\n",
        "    .addBands(elevation).addBands(sentinel2_ndvi).addBands(modis_et) \\\n",
        "    .addBands(soil_moisture).addBands(thermal) \\\n",
        "    .addBands(weighted_combined)'''\n",
        "\n",
        "\n",
        "# Sample feature values at archaeology site locations\n",
        "sampled_values = combined.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 5,  # Adjust scale depending on resolution needs\n",
        "    #\"properties\": [\"site_id\"],  # Add relevant properties\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "# Convert sampled values to FeatureCollection table\n",
        "def safe_set_coordinates(feature):\n",
        "    return feature.set({\n",
        "        \"Longitude\": feature.get(\"lon\"),\n",
        "        \"Latitude\": feature.get(\"lat\")\n",
        "    })\n",
        "\n",
        "#table = sampled_values.map(safe_set_coordinates)\n",
        "\n",
        "task = ee.batch.Export.table.toDrive(\n",
        "    collection=sampled_values, #table,\n",
        "    description=\"Archaeological_Site_Features\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task.start()  # Start export task\n",
        "\n",
        "### **Step 10: Apply K-Means Clustering**\n",
        "num_classes = 10\n",
        "'''training_points = combined.sample(**{\n",
        "    \"region\": southeast_brazil,\n",
        "    \"scale\": 10,\n",
        "    \"numPixels\": 500,\n",
        "    \"seed\": 42\n",
        "})'''\n",
        "training_points = combined.sample(\n",
        "    region=southeast_brazil,\n",
        "    # Default (False) is no geometries in the output.\n",
        "    # When set to True, each feature has a Point geometry at the center of the\n",
        "    # image pixel.\n",
        "    geometries=True,\n",
        "    numPixels = 500,\n",
        "    # The scale is not specified, so the resolution of the image will be used,\n",
        "    # and there is a feature for every pixel. If we give a scale parameter, the\n",
        "    # image will be resampled and there will be more or fewer features.\n",
        "    #\n",
        "    scale= 10,\n",
        ")\n",
        "# Extract longitude and latitude from geometry\n",
        "'''training_points = training_points.map(lambda feature:\n",
        "                     feature.set({\n",
        "                         \"Longitude\": feature.geometry().coordinates().get(0),\n",
        "                         \"Latitude\": feature.geometry().coordinates().get(1)\n",
        "                     })\n",
        "\n",
        "    )'''\n",
        "#training_points = training_points.filter(ee.Filter.notNull(['geometry']))\n",
        "print(training_points.first().getInfo())\n",
        "task1 = ee.batch.Export.table.toDrive(\n",
        "    collection=training_points,\n",
        "    description=\"Samples_random_collection\",\n",
        "    fileFormat=\"CSV\"\n",
        ")\n",
        "task1.start()\n",
        "clusterer = ee.Clusterer.wekaKMeans(num_classes).train(training_points)\n",
        "classified = combined.cluster(clusterer)\n",
        "\n",
        "### **Step 11: Validate Using Archaeological Sites**\n",
        "validation = classified.sampleRegions(**{\n",
        "    \"collection\": archaeology_points,\n",
        "    \"scale\": 10,\n",
        "    \"properties\": [\"class\"],\n",
        "    \"tileScale\": 2\n",
        "})\n",
        "\n",
        "### **Step 12: Visualization Parameters**\n",
        "vis_params_elevation = {\"bands\": [\"elevation\"], \"min\": 0, \"max\": 3000, \"palette\": [\"black\", \"white\"]}\n",
        "vis_params_ndvi = {\"bands\": [\"NDVI\"],\"min\": -0.6, \"max\": 0.6, \"palette\": [\"brown\", \"green\"]}\n",
        "vis_params_s1 = {\"bands\": [\"VV\"], \"min\": -20, \"max\": 0, \"gamma\": 1.4}\n",
        "vis_params_s2 = {\"bands\": [\"B12\", \"B11\", \"B8\"], \"min\": 0, \"max\": 3000, \"gamma\": 1.4}\n",
        "vis_params_modis_et = {\"bands\": [\"ET\"], \"min\": 0, \"max\": 300, \"palette\": [\"yellow\", \"green\", \"blue\"]}\n",
        "#vis_params_pml_et = {\"min\": 0, \"max\": 5, \"palette\": [\"orange\", \"red\", \"purple\"]}\n",
        "vis_params_soil = {\"bands\": [\"sm_surface\"], \"min\": 0, \"max\": 0.9, \"palette\": [\"red\", \"orange\", \"yellow\", \"green\"]}\n",
        "vis_params_thermal = {\"bands\": [\"ST_B10\"], \"min\": 270, \"max\": 320, \"palette\": [\"blue\", \"yellow\", \"red\"]}\n",
        "vis_params_classified = {\n",
        "    \"min\": 0,\n",
        "    \"max\": num_classes - 1,\n",
        "    \"palette\": [\"blue\", \"green\", \"yellow\", \"red\", \"purple\", \"orange\", \"brown\", \"cyan\", \"pink\", \"gray\"]\n",
        "}\n",
        "archaeology_vis = {\"color\": \"blue\", \"pointRadius\": 5}\n",
        "sample_vis = {\"color\": \"black\", \"pointRadius\": 4}\n",
        "### **Step 13: Create & Display Map**\n",
        "m = geemap.Map(center=[-3, -60], zoom=6)\n",
        "\n",
        "m.addLayer(elevation, vis_params_elevation, \"ASTER GDEM v3 Elevation\")\n",
        "m.addLayer(sentinel2_ndvi, vis_params_ndvi, \"High-Resolution NDVI (Sentinel-2)\")\n",
        "m.addLayer(mosaiced_sentinel1, vis_params_s1, \"Sentinel-1 Mosaic (Radar)\")\n",
        "m.addLayer(mosaiced_sentinel2, vis_params_s2, \"Sentinel-2 Mosaic (Optical)\")\n",
        "m.addLayer(modis_et, vis_params_modis_et, \"MODIS Evapotranspiration\")\n",
        "#m.addLayer(pml_et, vis_params_pml_et, \"PML Evapotranspiration\")\n",
        "m.addLayer(soil_moisture, vis_params_soil, \"Soil Moisture (SMAP)\")\n",
        "m.addLayer(thermal, vis_params_thermal, \"Thermal Infrared (Landsat)\")\n",
        "m.addLayer(classified, vis_params_classified, \"Classified Image (K-Means)\")\n",
        "m.addLayer(archaeology_points, archaeology_vis, \"Archaeology Points\")\n",
        "m.addLayer(training_points,sample_vis, \"sample Points\")\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ],
      "metadata": {
        "id": "fzW_axLw3MBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_points.size().getInfo())"
      ],
      "metadata": {
        "id": "MYzaT1RypkI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "with open(\"/content/Archaeological_Site_Features.csv\") as infile:\n",
        "    reader = csv.reader(infile, delimiter=\",\")\n",
        "    next(reader, None)\n",
        "    data3 =np.array(list(reader))"
      ],
      "metadata": {
        "id": "HXIes8X1vTJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dtype1 = np.dtype([('B11', np.float32),('B12', np.float32),('B8', np.float32),('VV', np.float32), ('Elevation', np.float32),('NDVI', np.float32), ('ET', np.float32),('SM', np.float32),('ST', np.float32),('Name', np.str_),('Latitude', np.float32),('Longitude', np.float32) ,('Class', np.int32)])\n",
        "#arr2 = np.zeros((len(data3),)).astype(dtype1)"
      ],
      "metadata": {
        "id": "vOINVvLZv7IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xjUbTAZxANv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtype1 = np.dtype([('B11', np.float32),('B12', np.float32),('B8', np.float32),('VV', np.float32),('Latitude', np.float32),('Longitude', np.float32),('Name', np.str_) ])\n",
        "arr2 = np.zeros((len(data3),)).astype(dtype1)"
      ],
      "metadata": {
        "id": "71r8hIS56gzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr2['B11'] = data3[:, 0]\n",
        "arr2['B12'] = data3[:, 1]\n",
        "arr2['B8'] = data3[:, 2]\n",
        "arr2['VV'] = data3[:, 3].astype(np.float32)\n",
        "arr2['Latitude'] = data3[:, 4]\n",
        "arr2['Longitude'] = data3[:, 5]\n",
        "arr2['Name'] = data3[:, 6]"
      ],
      "metadata": {
        "id": "-zEy86Xd7cRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr2['B11'] = data3[:, 0]\n",
        "arr2['B12'] = data3[:, 1]\n",
        "arr2['B8'] = data3[:, 2]\n",
        "arr2['ET'] = data3[:, 6]\n",
        "arr2['NDVI'] = data3[:, 5].astype(np.float32)\n",
        "arr2['ST'] = data3[:, 8].astype(np.float32)\n",
        "arr2['VV'] = data3[:, 3].astype(np.float32)\n",
        "arr2['Elevation'] = data3[:, 4]\n",
        "arr2['SM'] = data3[:, 7]\n",
        "arr2['Name'] = data3[:, 9]\n",
        "arr2['Latitude'] = data3[:, 10]\n",
        "arr2['Longitude'] = data3[:, 11]\n",
        "arr2['Class'] = data3[:, 12]"
      ],
      "metadata": {
        "id": "IH9rmZELz8ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, BatchNormalization, Input,Reshape\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "# Define custom loss function (MSE + small L1 penalty)\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) + 0.01 * tf.reduce_sum(tf.abs(y_pred))\n",
        "def r2_score(y_true, y_pred):\n",
        "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
        "    return 1 - (SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
        "# Define feature columns (exclude Name, Latitude, Longitude)\n",
        "#feature_columns = ['B11', 'B12', 'B8', 'VV', 'Elevation', 'NDVI', 'ET', 'SM', 'ST']\n",
        "feature_columns = ['B11', 'B12', 'B8', 'VV']\n",
        "X_structured = arr2[feature_columns]  # Extract features\n",
        "# Extract latitude & longitude properly from the structured array\n",
        "y = np.stack([arr2['Latitude'], arr2['Longitude']], axis=1).astype(np.float32)\n",
        "X = np.stack([X_structured[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "X_lstm1 = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "# Normalize features using MinMaxScaler\n",
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)  # Scale features\n",
        "\n",
        "# Normalize target (Latitude & Longitude) separately\n",
        "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Reshape X to fit LSTM input format (samples, timesteps, features)\n",
        "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 1 timestep\n",
        "\n",
        "# Build LSTM model for predicting Latitude & Longitude\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "'''model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(1, len(feature_columns))),\n",
        "    Dropout(0.3),  # Drops 30% of neurons randomly\n",
        "    LSTM(32),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dense(2)  # Predicts Latitude & Longitude\n",
        "])'''\n",
        "\n",
        "input_layer = Input(shape=(1, len(feature_columns)))\n",
        "x = Bidirectional(LSTM(256, return_sequences=True, kernel_regularizer=l2(0.001)))(input_layer)\n",
        "x = Dropout(0.4)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = LSTM(64, kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Reshape((1, 64))(x)  # Ensure 3D shape\n",
        "x = LSTM(32, kernel_regularizer=l2(0.001), return_sequences=False)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# Dense layers\n",
        "x = Dense(16, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "output_layer = Dense(2)(x)  # Predicts Latitude & Longitude\n",
        "# Build model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "# Compile model with MSE loss (regression task)\n",
        "#model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss=tf.keras.losses.Huber(delta=1.0), metrics=['mse'])#,loss=custom_loss, metrics= ['accuracy'])#['mse'])\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.Huber(delta=1.0),metrics= ['mse'])#, metrics=['r2_score'])\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H-DjtTkH-0bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract numeric values from the structured array\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "history = model.fit(X_lstm1, y, epochs=50, batch_size=32, validation_split=0.25, callbacks=[early_stopping])\n",
        "# Predict longitude & latitude for new site features\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot accuracy using MSE (lower is better)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mse'], label='Training mse')\n",
        "#plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_mse'], label='Validation mse')\n",
        "#plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "#plt.ylabel('Mean Squared Error')\n",
        "plt.ylabel('mse')\n",
        "plt.title('Training vs Validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zxissEqD_bVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n"
      ],
      "metadata": {
        "id": "5wJc4-eDmOgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "with open(\"/content/Samples_random_collection4.csv\") as infile:\n",
        "    reader = csv.reader(infile, delimiter=\",\")\n",
        "    next(reader, None)\n",
        "    data4 =np.array(list(reader))"
      ],
      "metadata": {
        "id": "Av6DAK4HmRF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype3 = np.dtype([('B11', np.float32),('B12', np.float32),('B8', np.float32), ('ET', np.float32),('NDVI', np.float32), ('ST', np.float32),('VV', np.float32),('Elevation', np.float32),('SM', np.float32) ])\n",
        "arr4 = np.zeros((len(data4),)).astype(dtype3)"
      ],
      "metadata": {
        "id": "A5dH4d3x9Ys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr4['B11'] = data4[:, 0]\n",
        "arr4['B12'] = data4[:, 1]\n",
        "arr4['B8'] = data4[:, 2]\n",
        "arr4['ET'] = data4[:, 3]\n",
        "arr4['NDVI'] = data4[:, 4].astype(np.float32)\n",
        "arr4['ST'] = data4[:, 5].astype(np.float32)\n",
        "arr4['VV'] = data4[:, 6].astype(np.float32)\n",
        "arr4['Elevation'] = data4[:, 7]\n",
        "arr4['SM'] = data4[:, 8]"
      ],
      "metadata": {
        "id": "G2jyJo949h0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define feature columns\n",
        "feature_columns = ['B11', 'B12', 'B8', 'VV', 'Elevation', 'NDVI', 'ET', 'SM', 'ST']\n",
        "\n",
        "# Prepare input data\n",
        "new_data_for_scaling = np.stack([arr4[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "\n",
        "# Normalize the new data\n",
        "#new_scaled = scaler.transform(new_data_for_scaling)\n",
        "\n",
        "# Reshape for LSTM input format\n",
        "#new_lstm_ready = new_scaled.reshape((new_scaled.shape[0], 1, new_scaled.shape[1]))\n",
        "new_lstm_ready = new_data_for_scaling.reshape((new_data_for_scaling.shape[0], 1, new_data_for_scaling.shape[1]))\n",
        "# Make predictions (now expecting longitude & latitude)\n",
        "predicted_coords_scaled = model.predict(new_lstm_ready)\n",
        "predicted_coords = scaler_y.inverse_transform(predicted_coords_scaled)\n",
        "# Convert predictions to a DataFrame\n",
        "results_df = pd.DataFrame(new_data_for_scaling, columns=feature_columns)  # Original input features\n",
        "\n",
        "# Add predicted longitude & latitude columns\n",
        "results_df['Predicted_Longitude'] = predicted_coords[:, 1]  # First column = longitude\n",
        "results_df['Predicted_Latitude'] = predicted_coords[:, 0]  # Second column = latitude\n",
        "\n",
        "# Save results to a CSV file\n",
        "results_df.to_csv('predicted_coordinates.csv', index=False)\n",
        "\n",
        "print(\"Predicted latitude and longitude saved to predicted_coordinates.csv\")\n"
      ],
      "metadata": {
        "id": "XVMxWUR4BUC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# True vs predicted coordinates\n",
        "true_lat = arr2[\"Latitude\"]\n",
        "true_lon = arr2[\"Longitude\"]\n",
        "pred_lat = predicted_coords[:, 0]  # Latitude\n",
        "pred_lon = predicted_coords[:, 1]  # Longitude\n",
        "print('true latitude',true_lat)\n",
        "print('true longitude',true_lon)\n",
        "print('predi latitude',pred_lat)\n",
        "print('predited longitude',pred_lon)\n",
        "# Calculate absolute errors for each point\n",
        "lat_errors = np.abs(true_lat - pred_lat)\n",
        "lon_errors = np.abs(true_lon - pred_lon)\n",
        "\n",
        "# Compute mean errors\n",
        "mean_lat_error = np.mean(lat_errors)\n",
        "mean_lon_error = np.mean(lon_errors)\n",
        "\n",
        "print(\"Individual Latitude Errors:\", lat_errors)\n",
        "print(\"Mean Latitude Error:\", mean_lat_error)\n",
        "print(\"Individual Longitude Errors:\", lon_errors)\n",
        "print(\"Mean Longitude Error:\", mean_lon_error)\n",
        "\n"
      ],
      "metadata": {
        "id": "J1vnPTtE7NKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot true locations\n",
        "plt.scatter(arr2[\"Longitude\"], arr2[\"Latitude\"], color='green', label='True Locations', marker='o')\n",
        "\n",
        "# Plot predicted locations\n",
        "plt.scatter(predicted_coords[:, 1], predicted_coords[:, 0], color='red', label='Predicted Locations', marker='x')\n",
        "\n",
        "# Connect true and predicted points to show displacement\n",
        "#for i in range(len(true_coords)):\n",
        "#    plt.plot([true_coords[i, 0], pred_coords[i, 0]], [true_coords[i, 1], pred_coords[i, 1]], color=\"gray\", linestyle=\"dashed\", alpha=0.5)\n",
        "\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.title(\"True vs. Predicted Locations of Archaeological Sites\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WeP1Hdti8Wjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype2 = np.dtype([('B11', np.float32),('B12', np.float32),('B8', np.float32), ('ET', np.float32),('NDVI', np.float32), ('ST', np.float32),('VV', np.float32),('Elevation', np.float32),('SM', np.float32),('Longitude', np.float32),('Latitude', np.float32) ])\n",
        "arr3 = np.zeros((len(data4),)).astype(dtype2)"
      ],
      "metadata": {
        "id": "EIrAavAMm1bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr3['B11'] = data4[:, 0]\n",
        "arr3['B12'] = data4[:, 1]\n",
        "arr3['B8'] = data4[:, 2]\n",
        "arr3['ET'] = data4[:, 3]\n",
        "arr3['NDVI'] = data4[:, 4].astype(np.float32)\n",
        "arr3['ST'] = data4[:, 5].astype(np.float32)\n",
        "arr3['VV'] = data4[:, 6].astype(np.float32)\n",
        "arr3['Elevation'] = data4[:, 7]\n",
        "arr3['SM'] = data4[:, 8]\n",
        "arr3['Longitude'] = data4[:, 9]\n",
        "arr3['Latitude'] = data4[:, 10]"
      ],
      "metadata": {
        "id": "cWy0fnbRmii9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction module\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "feature_columns = ['B11', 'B12', 'B8','VV', 'Elevation','NDVI','ET', 'SM', 'ST']\n",
        "# Prepare input data\n",
        "new_data_for_scaling = np.stack([arr3[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "\n",
        "# Normalize the new data\n",
        "new_scaled = scaler.transform(new_data_for_scaling)\n",
        "\n",
        "# Reshape for LSTM input format\n",
        "new_lstm_ready = new_scaled.reshape((new_scaled.shape[0], 1, new_scaled.shape[1]))\n",
        "\n",
        "# Make predictions\n",
        "predicted_classes = model.predict(new_lstm_ready)\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "results_df = pd.DataFrame(new_data_for_scaling, columns=feature_columns)  # Original input features\n",
        "results_df['Predicted_Class'] = predicted_classes.flatten()  # Add predictions\n",
        "results_df['Longitude'] = arr3['Longitude']\n",
        "results_df['Latitude'] = arr3['Latitude']\n",
        "# Save results to a CSV file\n",
        "results_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved to predictions.csv\")"
      ],
      "metadata": {
        "id": "tthiWXt6mDYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is based on the class\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# Define feature columns (exclude Name, Latitude, Longitude)\n",
        "def custom_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - y_true)) + 0.01 * tf.reduce_sum(tf.abs(y_pred))\n",
        "feature_columns = ['B11', 'B12', 'B8','VV', 'Elevation','NDVI','ET', 'SM', 'ST']\n",
        "X_structured = arr2[feature_columns]  # Extract features\n",
        "y = arr2[['Class']]  # Binary target (1 for archaeology, 0 for non-archaeology)\n",
        "X = np.stack([X_structured[col] for col in feature_columns], axis=1).astype(np.float32)\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape to fit LSTM input format: (samples, timesteps, features)\n",
        "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 1 timestep\n",
        "\n",
        "# Build LSTM model for binary classification\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(1, len(feature_columns))),\n",
        "    LSTM(32),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output: Binary classification (archaeological site or not)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss=custom_loss,metrics=['mse'])# loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "rwykACiX6k-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.stack([arr2['Class']], axis=1).astype(np.float32)\n",
        "model.fit(X_lstm, y, epochs=30, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "id": "dqwoVc7X8EhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}